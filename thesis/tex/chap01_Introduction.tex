%!TEX root = thesis.tex

\chapter{Introduction}
\label{chp:Introduction}

The deployment of \gls{ml} capabilities directly onto resource-constrained embedded devices—a field encapsulated by the term \gls{tinyml}—is fundamentally transforming the landscape of distributed intelligence \cite{duttaTinyMLMeetsIoT2021, immonenTinyMachineLearning2022, zaidiUnlockingEdgeIntelligence2022}. By enabling sophisticated data processing at the extreme edge of networks, this paradigm extends the reach of \gls{ai} to settings where inherent limitations in power, connectivity, or form factor previously precluded data-driven methods. The core innovation of \gls{tinyml} lies in its capacity to support not just on-device inference, but increasingly, localized learning processes. This empowers devices like \glspl{mcu} and low-power \glspl{sbc} to function as autonomous, intelligent agents. The strategic benefits are compelling: significantly reduced latency, inherently enhanced data privacy, and conserved network bandwidth, making \gls{tinyml} a key enabler for applications in industrial automation, healthcare, and autonomous systems \cite{tsoukasReviewEmergingTechnology2024, gillEdgeAITaxonomy2024}.

The primary impetus for the adoption of \gls{tinyml}, and consequently for the research presented in this thesis, stems from its capacity to function reliably in environments where continuous connectivity or abundant power cannot be assumed. This capability elevates \gls{tinyml} from a technical optimization to a strategic enabler of ubiquitous, embedded intelligence. The demanding operational conditions for which the framework developed in this research is designed are well exemplified by a Mars rover scenario. In such a setting, a prototype equipped with a camera would classify objects (e.g., rocks) but could only communicate with mission control during scheduled windows. During extensive offline periods, this rover would need to operate reliably despite changing visual conditions and strict energy budgets, underscoring the critical requirement for on-device intelligence to manage its \gls{ml} models effectively. This research, therefore, is driven by the necessity to develop robust methods and a comprehensive framework for managing the operational lifecycle of such \gls{tinyml} systems, with a particular focus on enabling greater model autonomy and addressing the inherent operational challenges in these demanding environments. 

\section{Problem}
\label{sec:problem}

Deploying \gls{ml} capabilities at the network edge offers distinct advantages such as reduced latency, preserved privacy, and lowered bandwidth requirements \cite{capogrossoMachineLearningOrientedSurvey2024, tsoukasReviewEmergingTechnology2024}. However, the \gls{lcm} of such models, which encompasses deployment, monitoring, update, and eventual retirement, remains exceptionally challenging. These challenges are particularly acute on devices that operate with only kilobytes of memory, milliwatts of power, and intermittent connectivity \cite{szydloManagementTinyMLEnabled2024, disabatoTinyMachineLearning2024, antoniniTinyMLOpsFrameworkOrchestrating2022}. Most existing \gls{mlops} frameworks assume a cloud-centric setting characterized by abundant resources and stable communication links \cite{kreuzbergerMachineLearningOperations2023, johnMLOpsFrameworkMaturity2021, burgueno-romeroOpenSourceMLOps2025}. These fundamental assumptions do not hold for \glspl{mcu} and small \glspl{sbc}, where memory is measured in the range of \SI{10}{\kilo\byte} to \SI{2}{\mega\byte} rather than gigabytes, and where power budgets often lie below \SI{100}{\milli\watt} \cite{banburyBenchmarkingTinyMLSystems2021, linMCUNetTinyDeep2020a, zaidiUnlockingEdgeIntelligence2022}. This fundamental mismatch leads to a conspicuous absence of mature operational practices tailored to the unique constraints of \gls{tinyml} devices, creating an operational void that hinders their long-term reliability, maintainability, and adaptability.

As a consequence of this operational void and the limitations of conventional \gls{mlops} approaches, on-device applications often depend heavily on cloud infrastructure for critical \gls{lcm} tasks such as monitoring, retraining, and redeployment \cite{kreuzbergerMachineLearningOperations2023}. Such reliance severely impedes use cases that require autonomous operation or must function in areas with unreliable or non-existent connectivity. This creates an ``autonomy gap'' —a significant disconnect between the aspiration for autonomous edge intelligence and the current operational capabilities that frequently tether these devices to centralized cloud services. This gap is not merely a matter of resource limitations but reflects the need for an entirely different operational paradigm suited to the edge.

Research to date, while addressing isolated parts of the \gls{tinyml} pipeline, such as on-device training \cite{renOndeviceOnlineLearning2024, disabatoIncrementalOnDeviceTiny2020} , efficient update mechanisms \cite{sudharsanOTATinyMLAirDeployment2022, huangRIOTMLToolkitOvertheair2024a} , and lightweight monitoring \cite{antoniniTinyMLOpsFrameworkOrchestrating2022, minSensiXBringingMLOps2023} , does not yet converge on a cohesive, end-to-end framework for autonomous \gls{lcm}. The literature analysis conducted as part of this research further confirms this fragmentation, revealing that many existing solutions are conceptual proposals lacking empirical validation, or they focus on narrow use cases and partial lifecycle workflows, often maintaining significant dependencies on external infrastructures. This fragmented nature of current research means that even if individual components are optimized, integrating them into a truly autonomous, end-to-end system remains a significant unsolved challenge. The burgeoning complexity arising from \gls{tinyml}'s increasing success and deployment scale naturally gives rise to the imperative for systematic \gls{mlops} specifically re-engineered for the \gls{tinyml} context. Therefore, dedicated research into TinyMLOps, aimed at developing holistic and validated solutions for autonomous on-device \gls{lcm}, is not merely beneficial but essential for realizing the full potential of intelligent edge computing. This thesis directly addresses this critical need. 

\section{Research Objectives}
\label{sec:objective}

To address the critical operational gap identified, this thesis sets out to chart a path toward a robust and principled approach for autonomous \gls{tinymlops}. The research pursues a sequence of interconnected objectives designed to systematically build from a foundational understanding of the problem space to a validated, practical solution.

It begins by systematically mapping the current landscape to deconstruct existing architectures, methodologies, and practices. This initial exploration seeks to understand not only what is currently possible but also to pinpoint the precise limitations that hinder true on-device autonomy. Subsequently, the investigation aims to distill the core strategies and frameworks that enable decentralized intelligence, focusing on techniques that explicitly minimize reliance on cloud infrastructure.

Building on this foundational knowledge, the central objective is to design and engineer a novel \gls{tinymlops} ecosystem, architected from the ground up for the unique constraints of edge hardware. This involves the creation of a tangible framework that embodies the principles of on-device lifecycle management. Finally, because a conceptual design alone is insufficient, the research culminates in the empirical evaluation of this ecosystem. This final objective is to quantify the framework's real-world impact on deployment complexity, resource consumption, and performance trade-offs. The ultimate goal is to deliver a validated and principled framework that empowers developers to build and manage the next generation of truly autonomous intelligent devices.

\section{Contribution}
\label{sec:contribution}

The scientific contribution of this thesis is a holistic, empirically-grounded solution to the challenge of autonomous operations in \gls{tinyml}. This work fundamentally re-imagines established \gls{mlops} principles for the unique constraints of edge computing. Grounded in a systematic analysis of the research landscape that identifies a critical gap between the ambition for on-device intelligence and the reality of cloud-dependent operations, this thesis introduces a novel ``autonomy-first'' \gls{tinymlops} ecosystem. The core of this contribution is the design of \gls{tinylcm}, an on-device framework featuring key innovations like integrated, unsupervised drift detection and robust state management, which is complemented by the optional server-side TinySphere. Crucially, the work extends beyond pure theory by providing a rigorous empirical validation of the framework's feasibility and effectiveness on resource-constrained hardware. The result is not merely a conceptual model, but a validated prototype with concrete performance benchmarks, demonstrating a practical pathway toward creating more resilient, adaptive, and truly autonomous intelligent systems.

\section{Outline}
\label{sec:structure-of-thesis}

The remainder of the thesis proceeds as follows. Chapter~\ref{chp:Related_Work} establishes the theoretical foundation by examining three fundamental domains: \gls{mlops}, resource-constrained systems, and \gls{tinyml}. This chapter also analyzes existing research at their intersection and identifies key limitations in current approaches to \gls{lcm} for autonomous \gls{tinyml} systems.

Chapter~\ref{chp:Research_Design} outlines the methodological approach, defines the research questions, and details the literature review process employed to identify current practices and research gaps. The chosen approach follows established guidelines for systematic reviews in software engineering, complemented by snowballing techniques. The chapter also introduces the case study methodology used for framework evaluation, grounded in \gls{dsr} principles.

Chapter~\ref{chp:Research_Results} presents comprehensive findings from the systematic analysis of existing literature, encompassing quantitative trends, architectural patterns, and implementation strategies. The chapter then maps the current state of research across multiple dimensions. These results reveal notable gaps in approaches for autonomous \gls{tinyml} \gls{lcm}.

Chapter~\ref{chp:Framework} introduces \gls{tinylcm}, a framework designed for autonomous \gls{lcm} of \gls{ml} applications on resource-constrained devices. The framework synthesizes insights from the literature review and addresses identified limitations in existing approaches, with a particular focus on minimizing cloud dependencies while maintaining core \gls{mlops} capabilities.

Chapter~\ref{chp:Evaluation} evaluates the effectiveness of the proposed framework in enabling autonomous \gls{lcm}, critically examines the research findings, and considers limitations and threats to validity.

Chapter~\ref{chp:Discussion} synthesizes and interprets the key findings of the study. It discusses their broader implications, highlights the novelty of the research, explicitly answers the posed \glspl{rq}, and critically examines the study's limitations. Chapter~\ref{chp:Conclusion} then summarizes the principal contributions and overall implications of the research, while Chapter~\ref{chp:Future_Work} outlines promising directions for further investigation and development in the field of autonomous \gls{tinyml}.