%!TEX root = thesis.tex

\chapter{Background and Related Work}
\label{chp:Related_Work}

The present chapter provides the conceptual and technical basis for the proposed research. Section~\ref{sec:background_main} surveys essential concepts—\gls{mlops}, resource-constrained systems, and \gls{tinyml}—which underpin the remainder of the thesis. Subsequently, Section~\ref{sec:relatedWork} positions the study within the current literature. It summarizes prior solutions and identifies open challenges that motivate the proposed \gls{tinymlops} framework.

~\\
\vfill
\minitoc
\clearpage

\section{Background}
\label{sec:background_main}

Understanding the challenges and opportunities in managing \gls{ml} models on embedded devices requires consideration of several key areas. These include established practices in \gls{mlops}, the inherent limitations of resource-constrained systems, and specific optimization techniques developed under the \gls{tinyml} paradigm. The following subsections detail these areas, providing the necessary context for the subsequent discussion of related work and ensuing chapters.

\subsection{MLOps}
\label{subsec:mlops}

The growing integration of \gls{ai} has increased the importance of sound \gls{mlops} frameworks for managing \gls{ml} models and systems efficiently \cite{burgueno-romeroOpenSourceMLOps2025}. Rooted in \gls{devops} principles, \gls{mlops} incorporates practices tailored to the specific demands of \gls{ml} workflows \cite{johnMLOpsFrameworkMaturity2021}. This adaptation addresses the challenge of transitioning \gls{ml} models from research and experimental settings to scalable, dependable, and maintainable production environments—a typically complex process \cite{burgueno-romeroOpenSourceMLOps2025, testiMLOpsTaxonomyMethodology2022, kreuzbergerMachineLearningOperations2023}.
Essentially, \gls{mlops} strives to automate the entire lifecycle of \gls{ml} models. Its primary goal is to address the complexity, time expenditure, and scalability barriers inherent in manual \gls{ml} pipeline management \cite{burgueno-romeroOpenSourceMLOps2025, johnMLOpsFrameworkMaturity2021}.

Several core principles underpin the development of reliable, high-performance \gls{ml} systems. Specifically, \gls{ci}, \gls{cd}, and \gls{ct} enable rapid feedback loops, iterative refinements, and reduced manual effort \cite{burgueno-romeroOpenSourceMLOps2025, johnMLOpsFrameworkMaturity2021, karamitsosApplyingDevOpsPractices2020, kreuzbergerMachineLearningOperations2023}. Additionally, reproducibility, traceability, and version control of code, data, and models foster auditability and consistency in \gls{ml} experimentation \cite{burgueno-romeroOpenSourceMLOps2025, kreuzbergerMachineLearningOperations2023}. Furthermore, effective collaboration and communication among data scientists, developers, and operations teams reduce organizational silos and cultivate shared responsibility \cite{burgueno-romeroOpenSourceMLOps2025, johnMLOpsFrameworkMaturity2021, kreuzbergerMachineLearningOperations2023}. To implement these principles, \gls{mlops} architectures integrate technical components that operate cohesively. Examples include model registries for central model governance, pipeline orchestration for workflow automation, model serving platforms for deployment, and monitoring systems for performance assessment \cite{burgueno-romeroOpenSourceMLOps2025, kreuzbergerMachineLearningOperations2023}. These components are typically built with open-source and cloud-native technologies, such as Kubernetes, to leverage scalability, flexibility, and community contributions \cite{burgueno-romeroOpenSourceMLOps2025}.

Researchers have proposed various taxonomies to structure the \gls{mlops} landscape. For instance, Testi et al. \cite{testiMLOpsTaxonomyMethodology2022} delineate three primary dimensions: data management, modeling, and operationalization. Their delineation highlights considerations throughout the \gls{ml} lifecycle, from data acquisition to deployment and continuous monitoring. Alternatively, Kreuzberger et al. \cite{kreuzbergerMachineLearningOperations2023} categorize \gls{mlops} into principles, technical components, and roles within an integrated architecture. Their work underscores the importance of technical infrastructure, organizational dynamics, and human expertise in \gls{mlops} adoption. Such differing perspectives indicate that \gls{mlops} extends beyond a single toolkit or practice. Indeed, it necessitates a cultural and organizational transformation, reminiscent of \gls{devops}, to fully realize its advantages \cite{johnMLOpsFrameworkMaturity2021, karamitsosApplyingDevOpsPractices2020}.

\subsection{Resource-Constrained Systems}
\label{subsec:resourceConstrained}

Resource-constrained systems are computing platforms characterized by limited processing power, memory, and energy capacity \cite{brancoMachineLearningResourceScarce2019, banburyBenchmarkingTinyMLSystems2021}. Such limitations necessitate highly efficient operations under conditions often determined by cost, physical size, and power availability \cite{duttaTinyMLMeetsIoT2021}. Commonly found in embedded environments, these systems typically employ \glspl{mcu} or low-power \glspl{sbc}, prioritizing power efficiency and cost-effectiveness over raw computational throughput \cite{duttaTinyMLMeetsIoT2021, capogrossoMachineLearningOrientedSurvey2024}. The platforms are utilized across multiple industries, including \gls{iot} devices, wearables, industrial automation, and remote sensing, where real-time processing and energy minimization are often paramount \cite{capogrossoMachineLearningOrientedSurvey2024}.

A notable feature of these systems is their constrained computational performance. \glspl{mcu} generally operate at clock speeds ranging from approximately \SI{40}{\mega\hertz} to \SI{400}{\mega\hertz}, limiting the feasibility of computationally intensive applications \cite{capogrossoMachineLearningOrientedSurvey2024,linMCUNetTinyDeep2020a}. They also exhibit stringent memory limits: \glspl{mcu} commonly possess only a few \glspl{kb} or \glspl{mb} of storage. This imposes tight restrictions on software architectures and the feasibility of onboard computations \cite{capogrossoMachineLearningOrientedSurvey2024,linMCUNetTinyDeep2020a}. Furthermore, power consumption is a dominant design consideration, as many such devices operate autonomously on small batteries. Their power budgets typically lie in the low \si{\milli\watt} range, mandating high energy efficiency across both hardware selection and software design \cite{immonenTinyMachineLearning2022, capogrossoMachineLearningOrientedSurvey2024}.

To better contextualize resource-constrained systems, a comparison with cloud and conventional edge computing paradigms is useful. Cloud computing offers extensive computational resources, scalable storage, and powerful infrastructures, making it well-suited for large-scale \gls{ml}, data analytics, and high-demand simulations \cite{changSurveyRecentAdvances2021}. However, cloud-based solutions inherently rely on network connectivity. This dependency introduces latency and a reliance on remote servers, potentially limiting their suitability for scenarios requiring real-time responsiveness or adhering to strict energy budgets. In contrast, resource-constrained systems prioritize on-device computation with minimal power usage, albeit at the cost of notably lower processing power and memory. While cloud solutions emphasize performance and scalability, these embedded devices focus on energy-conscious operation and prolonged battery life \cite{banburyBenchmarkingTinyMLSystems2021, changSurveyRecentAdvances2021}. Conventional edge computing aims to mitigate cloud latency by shifting computation closer to data sources. Such edge devices, including industrial gateways or more capable \glspl{sbc}, generally surpass \glspl{mcu} in computational power but remain physically closer to the data source than cloud infrastructures \cite{capogrossoMachineLearningOrientedSurvey2024}. Resource-constrained systems, as discussed here, often inhabit the far periphery of this spectrum. In these environments, ultra-low energy usage can supersede computational flexibility. Devices frequently use bare-metal programming or a \gls{rtos} to maximize energy performance, unlike typical edge devices that might run full operating systems \cite{brancoMachineLearningResourceScarce2019}.

\subsection{TinyML}
\label{subsec:tinyml}

\Gls{tinyml} has emerged as an evolving sub-field of \gls{ai}, driven by the need to execute \gls{ml} models on resource-constrained devices—a trend that has become increasingly prevalent \cite{zaidiUnlockingEdgeIntelligence2022, capogrossoMachineLearningOrientedSurvey2024}. Conceptually, \gls{tinyml} involves the tight integration of hardware and software required to embed \gls{ml} models—often compact deep neural networks—on low-power platforms such as \glspl{mcu} or small \glspl{sbc} \cite{duttaTinyMLMeetsIoT2021, tsoukasReviewEmergingTechnology2024}. On-device inference can reduce latency, strengthen data privacy, and lower power consumption as well as bandwidth demand \cite{immonenTinyMachineLearning2022, tsoukasReviewEmergingTechnology2024, gillEdgeAITaxonomy2024}. Consequently, \gls{tinyml} adoption is increasing in diverse sectors including healthcare, automotive, agriculture, and industrial automation \cite{tsoukasReviewEmergingTechnology2024, gillEdgeAITaxonomy2024}.

A primary motivation for \gls{tinyml} is energy optimization at the network edge \cite{tekinReviewOndeviceMachine2024, tsoukasReviewEmergingTechnology2024}. By executing inference locally, devices can eliminate continuous cloud interaction, thereby reducing total energy consumption \cite{duttaTinyMLMeetsIoT2021}. Achieving such efficiency on platforms that may offer only \SIrange{10}{100}{\kilo\byte} of memory, methods like quantization, pruning, knowledge distillation, and \gls{nas} are necessary \cite{capogrossoMachineLearningOrientedSurvey2024, tekinReviewOndeviceMachine2024}. Beyond energy savings, localized processing offers additional advantages. It enhances real-time performance and alleviates bandwidth constraints by obviating the need for large-scale data transmission \cite{tsoukasReviewEmergingTechnology2024, gillEdgeAITaxonomy2024}. Concurrently, system security and privacy are often improved, as sensitive data can remain on edge devices, reducing exposure to external networks \cite{immonenTinyMachineLearning2022, duttaTinyMLMeetsIoT2021}.

Despite these benefits, \gls{tinyml} faces notable technical challenges due to hardware limitations. \gls{mcu}s typically provide only tens or hundreds of \glspl{kb} of memory alongside demanding power constraints \cite{zaidiUnlockingEdgeIntelligence2022, immonenTinyMachineLearning2022}. Achieving acceptable performance thus requires advanced optimization strategies that balance accuracy, inference speed, and energy usage \cite{capogrossoMachineLearningOrientedSurvey2024, szydloManagementTinyMLEnabled2024, disabatoTinyMachineLearning2024}. Common solutions include quantization (lowering numerical precision), pruning (removing redundant model connections), and knowledge distillation (transferring insights from larger ``teacher'' models) \cite{tekinReviewOndeviceMachine2024, gillEdgeAITaxonomy2024}. Additionally, \gls{nas} can automate the development of compact model designs tailored to stringent edge requirements \cite{tsoukasReviewEmergingTechnology2024}. In parallel, hardware-level optimizations, such as specialized accelerators, can provide further gains in performance and energy efficiency \cite{gillEdgeAITaxonomy2024}.

Subsequently, several open-source frameworks aim to simplify \gls{tinyml} development. \Gls{tflm}, Edge Impulse, and uTensor support model conversion, optimization, and device deployment, abstracting lower-level intricacies \cite{duttaTinyMLMeetsIoT2021, rayReviewTinyMLStateoftheart2022, tekinReviewOndeviceMachine2024, tsoukasReviewEmergingTechnology2024}. Meanwhile, various taxonomies clarify the scope of \gls{tinyml} and Edge AI. For example, Gill et al. \cite{gillEdgeAITaxonomy2024} propose a wide-ranging taxonomy of Edge AI, which encompasses \gls{tinyml}. Their classification details eleven sub-topics, such as infrastructure, application architecture, \gls{iot} use cases, methods, resource management, and \gls{ml} model sizing, alongside heterogeneity, security, scheduling, container migration, and container scaling. Such a perspective highlights the architectural and operational intricacies of \gls{tinyml} within the broader Edge AI landscape. In contrast, Capogrosso et al. \cite{capogrossoMachineLearningOrientedSurvey2024} organize \gls{tinyml} into three domains: model optimization, model design, and learning algorithms. Their focus lies on algorithm selection, network configuration, and optimization trade-offs. While each taxonomy adopts a distinct vantage point, they collectively underscore the multifaceted complexity of \gls{tinyml}.

\section{Related Work}
\label{sec:relatedWork}

Having established the foundational concepts of \gls{mlops}, resource-constrained systems, and \gls{tinyml}, the present section contextualizes the proposed \gls{tinymlops} framework. This is achieved by discussing key existing research. Related work is grouped into several categories: secondary studies synthesizing research on \gls{tinyml} and \gls{mlops}; illustrative use-case implementations; foundational \gls{tinyml} frameworks; and works targeting \gls{tinymlops} specifically. The review process helps to identify the current state-of-the-art and remaining gaps that the thesis aims to address.

Capogrosso et al. \cite{capogrossoMachineLearningOrientedSurvey2024} provide an \gls{slr} and mapping study adhering to PRISMA guidelines. Their work offers a broad overview of \gls{tinyml} with an emphasis on \gls{ml} algorithms designed for resource-constrained hardware. The survey methodically explores workflows, model optimization taxonomies, hardware/software tools, and future research avenues. Similarly, Ray \cite{rayReviewTinyMLStateoftheart2022} reviews \gls{tinyml}, covering its historical evolution, available toolchains, enabling technologies, deployment frameworks, and emerging directions. In another review, Tsoukas et al. \cite{tsoukasReviewEmergingTechnology2024} chart the growth of \gls{tinyml}, examining optimization methods, benefits, practical difficulties, tool stacks, and educational opportunities. Tekin et al. \cite{tekinReviewOndeviceMachine2024} investigate energy constraints in on-device \gls{ml} for the \gls{iot}. They propose a taxonomy of energy-optimized \gls{ml} models and highlight pending challenges. In parallel, Leroux et al. \cite{lerouxTinyMLOpsOperationalChallenges2022a} address the operational challenges of deploying \gls{tinyml}. The authors point beyond computational bottlenecks to issues of security, monitoring, and management, further motivating the need for specialized frameworks such as \gls{tinymlops}.

Regarding real-world applications, the literature highlights various domain-specific \gls{tinyml} deployments. Lin et al. \cite{linTinyMachineLearning2024} present a real-time bolt-defect detection system for climbing robots built on resource-constrained hardware, leveraging the FOMO algorithm. Their results illustrate the viability of computer vision tasks on energy-limited platforms. Another example by Azevedo et al. \cite{azevedoDetectingFaceMasks2023a} demonstrates a face mask detection solution using \gls{tinyml}. This solution employs transfer learning and MobileNet on an Arduino Nano board.

Concerning frameworks designed to enable \gls{tinyml}, David et al. \cite{davidTensorFlowLiteMicro2021} describe the foundation of \gls{tflm}. This open-source approach directly addresses resource scarcity in embedded devices and the ecosystem's fragmentation challenges. \gls{tflm} offers a flexible, interpreter-based workflow and a variety of optimization pathways tailored to embedded hardware. Pavan et al. \cite{pavanTyBoxAutomaticDesign2024} propose TyBox, a toolkit for the automated design and generation of incremental on-device \gls{tinyml} models. TyBox focuses on continuous learning under constrained resources and automatically produces C++ code for straightforward \gls{mcu} deployment. Sudharsan et al. \cite{sudharsanEdge2TrainFrameworkTrain2020} introduce Edge2Train, a framework specifically supporting offline, on-device \gls{svm} model training. It provides reference C++ source code for different \gls{mcu} platforms.

To address the comprehensive \gls{lcm} of \gls{tinyml} models, \gls{tinymlops} frameworks have started to emerge. Min et al. \cite{minSensiXBringingMLOps2023} propose SensiX++, a multi-tenant runtime augmented by \gls{mlops} for embedded sensory platforms. SensiX++ emphasizes declarative orchestrations, resource-aware model isolation, and external data operations to improve multi-model serving on resource-constrained devices. Antonini et al. \cite{antoniniTinyMLOpsFrameworkOrchestrating2022} formalize a \gls{tinymlops} framework by customizing classical \gls{mlops} paradigms for far-edge environments. Their work spotlights automated re-deployment and adaptation cycles for small devices. Banbury et al. \cite{banburyEdgeImpulseMLOps2023} present Edge Impulse, an \gls{mlops} platform centered on \gls{tinyml}. It provides an end-to-end pipeline—from data gathering to on-device deployment—with integrated AutoML features and hardware-centric optimizations.

Despite these contributions, most existing solutions concentrate on narrow use cases, partial lifecycle workflows, or depend heavily on external infrastructures. The research presented in this thesis diverges by advocating a device-centric \gls{tinymlops} solution. Such an approach ensures that central lifecycle phases of \gls{tinyml} models can be executed locally on resource-constrained hardware. By developing an end-to-end framework, the present work aims to address a broader set of application demands and support in-situ model evolution without extensive reliance on cloud resources. A consistent on-device perspective, therefore, is intended to set the proposed framework apart from other initiatives in the \gls{tinymlops} field.